{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The problem of classification deals with *categorical* data. In this problem, we wish to identify a set of data whether they belong to a particular class of category. For example, a given text message from an email, we would like to classify if it is a spam or not a spam. Another example would be given some measurement of cancer cells we wish to classify if it is benign or malignant. In this section we will learn logistic regression to solve this classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "\n",
    "Let's take an example of breast cancer classification problem. Let's say depending on the cell size, an expert can identify if the cell is benign or malignant. We can plot something like the following figure.\n",
    "\n",
    "![](https://www.dropbox.com/s/drkggfjxttwjjfu/cancer_cell_plot.png?raw=1)\n",
    "\n",
    "In the y-axis, value of 1 means it is a malignant cell while value of 0 means it is benign. The x-axis can be considered as a normalized size of the cell with mean 0 and standard deviation of 1 (recall z-normalization).\n",
    "\n",
    "If we can model this plot as a function $p(x)$, we can set the following criteria to classify the cells. For example, we will predict it is malignant if $p(x) \\geq 0.5$, otherwise, it is benign. This means we need a function where we can model the data in a step wise manners and fulfills the following:\n",
    "\n",
    "$$0 \\leq p(x) \\leq 1$$\n",
    "\n",
    "where $p(x)$ is the probability that a cell with feature $x$ is a malignant cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the function that we can use that have this step-wize shape and the above properties is a logistic function. A logistic function can be written as.\n",
    "\n",
    "$$y = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of a logistic function looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe8f86ef690>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array(range(-10,11))\n",
    "y = 1/(1+np.exp(-x))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write our hypothesis as follows.\n",
    "\n",
    "$$p(x) = \\frac{1}{1 + e^{-\\beta^T x}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this notes we tend to omit the *hat* symbol to indicate it is the estimated parameters as in the previous notes. We will just indicate the estimated parameters as $\\beta$ instead of $\\hat{\\beta}$. In the above equation, $\\beta^Tx$ can be written as\n",
    "\n",
    "$$\\beta_0 x_0 + \\beta_1 x_1$$\n",
    "\n",
    "when $x_0 = 1$, the above equation is simply the straight line equation of linear regression.\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_1$$\n",
    "\n",
    "This is the case when we only have one feature $x_1$. If we have more than one feature, we should write $\\beta^Tx$ as follows.\n",
    "\n",
    "$$\\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_n x_n$$\n",
    "\n",
    "The above relationship shows that we can map the value of linear regression into a new function with a value from 0 to 1. This new function $p(x)$ can be considered as *an estimated probability* that $y = 1$ on input $x$. For example, if $p(x) = 0.7$ this means that 70% chance it is malignant. We can then use the following boundary conditions:\n",
    "- y = 1 (malignant) if $p(x) \\geq 0.5$\n",
    "- y = 0 (benign) if $p(x) < 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above conditions also means that we can classify $y=1$ when $\\beta^T x \\geq 0$ and $y = 0$ when $\\beta^T x < 0$. We can draw these boundary conditions.\n",
    "\n",
    "![](https://www.dropbox.com/s/ncflm4853mdy8es/decision_boundary.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we indicated the predicted label $y$ with the orange colour. We see that when $p(x)\\geq 0.5$, the data is marked as $y=1$ (orange). On the other hand, when $p(x) \\leq 0.5$, the data is marked as $y=0$ (orange). The thick black line shows the decision boundary for this particular example. \n",
    "\n",
    "How do we get this boundary decision. Once we found the estimated values for $\\beta$, we can find the value of $x$ which gives $\\beta^Tx = 0$. You will work on computing the parameters $\\beta$ in the problem set. For now, let's assume that you manage to find the value of $\\beta_0 = -0.56$ and $\\beta_1 = 1.94$. The equation $\\beta^T x = 0 $ can be written as follows.\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x = 0$$\n",
    "\n",
    "We can then substitute the values for $\\beta$ into the equation.\n",
    "\n",
    "$$-0.56 + 1.94 x = 0$$\n",
    "$$x = 0.29 \\approx 0.3$$\n",
    "\n",
    "From the figure above, this fits where the thick line is, which is at around 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Similar to linear regression, our purpose here is to find the parameters $\\beta$. To do so, we will have to minimize some cost function using optimization algorithm. \n",
    "\n",
    "For logistic regression, we will choose the following cost function.\n",
    "\n",
    "$$J(\\beta) = \\frac{1}{m} \\Sigma_{i=1}^m \\left\\{ \\begin{matrix}\n",
    "-\\log(p(x)) & \\text{ if } y = 1\\\\\n",
    "-\\log(1 - p(x)) & \\text{ if } y = 0\n",
    "\\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to understand the term inside the bracket intuitively. Let's see the case when $y=1$. In this case, the cost term is given by:\n",
    "\n",
    "$$-\\log(p(x))$$\n",
    "\n",
    "The cost is 0 if $y = 1$ and $p(x) = 1$ because $-\\log(z)$ is 0 when $z=1$. Moreover, as $p(x) \\rightarrow 0$, the cost will reach $\\infty$. [See plot by wolfram alpha](https://www.wolframalpha.com/input/?i=-log%28x%29+from+0+to+1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, when $ y = 0$, the cost term is given by:\n",
    "\n",
    "$$-\\log(1-p(x))$$\n",
    "\n",
    "In this case, the cost is 0 when $p(x) = 0$ but it reaches $\\infty$ when $p(x) \\rightarrow 1$. [See plot by wolfram alpha](https://www.wolframalpha.com/input/?i=-log%281-x%29+from+0+to+1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the overall cost function for all the data points from $i=1$ to $m$ as follows.\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m y^i \\log(p(x^i)) + (1 - y^i) \\log(1 - p(x^i))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when $y^i = 1$, the function reduces to\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m  \\log(p(x^i)) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and when $y^i = 0$, the function reduces to\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m  \\log(1 - p(x^i))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We can find the parameters $\\beta$ again by using the gradient descern algorithm to perform:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "min & J(\\beta)\\\\\n",
    "\\beta & \\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update functions for the parameters are given by\n",
    "\n",
    "$$\\beta_j = \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} J(\\beta)$$\n",
    "\n",
    "The derivative of the cost function is given by\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma_{i=1}^m \\left(p(x)-y^i  \\right)x_j^i$$\n",
    "\n",
    "See the appendix for the derivation. We can substitute this in to get the following update function.\n",
    "\n",
    "$$\\beta_j = \\beta_j - \\alpha \\frac{1}{m}\\Sigma_{i=1}^m \\left(p(x)-y^i  \\right)x_j^i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Derivation of Logistic Regression Derivative \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $\\frac{\\partial}{\\partial \\beta_j}J(\\beta)$, where\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m y^i \\log(p(x^i)) + (1 - y^i) \\log(1 - p(x^i))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our derivation, we will consider each case when $y=1$ and when $y=0$. When $y=1$, the cost function is given by\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m  \\log(p(x^i)) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivating this with respect to $\\theta$ is\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma \\frac{1}{p(x)}\\frac{\\partial}{\\partial \\beta}p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the expression for the hypothesis is\n",
    "\n",
    "$$p(x) = \\frac{1}{1 + e^{-\\beta^T x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of this is given by\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j} p(x) = - \\frac{1}{(1 + e^{-\\beta^T x})^2} \\times -x_j \\times e^{-\\beta^T x}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j} p(x) =  \\frac{x_j e^{-\\beta^T x}}{(1 + e^{-\\beta^T x})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then now substitute this back \n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma (1 + e^{-\\beta^T x}) \\frac{x_j e^{-\\beta^T x}}{(1 + e^{-\\beta^T x})^2}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma  \\frac{x_j e^{-\\beta^T x}}{(1 + e^{-\\beta^T x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written as\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma  p(x) x_j e^{-\\beta^T x}$$\n",
    "This is for the case of $y = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for $y = 0$, the cost function is given by\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m  \\log(1 - p(x^i))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivating this with respect to $\\theta$ gives\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma \\frac{1}{1 - p(x)}\\frac{\\partial}{\\partial \\beta}p(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting expression for the hypothesis function and its derivative  gives us\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma \\frac{1}{1 - \\frac{1}{1 + e^{-\\beta^T x}}}  \\frac{x_j e^{-\\beta^T x}}{(1 + e^{-\\beta^T x})^2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma \\frac{1 + e^{-\\beta^T x}}{e^{-\\beta^T x}}  \\frac{x_j e^{-\\beta^T x}}{(1 + e^{-\\beta^T x})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma \\frac{x_j}{(1+e^{\\beta^T x})} $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma p(x) x_j$$\n",
    "This is for $y = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining for both cases $y=0$ and $y=1$, we have\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma_{i=1}^m y^i p(x) x_j e^{-\\beta^T x} + (y^i - 1)  p(x) x_j^i$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma_{i=1}^m y^i p(x) x_j e^{-\\beta^T x} + y^i   p(x) x_j - p(x) x_j^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma_{i=1}^m \\left(y^i p(x)(1 + e^{-\\beta^T x})  - p(x) \\right)x_j^i$$\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = -\\frac{1}{m}\\Sigma_{i=1}^m \\left(y^i   - p(x) \\right)x_j^i$$\n",
    "$$\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\frac{1}{m}\\Sigma_{i=1}^m \\left(p(x)-y^i  \\right)x_j^i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
